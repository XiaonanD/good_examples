{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "> #### Feature Selection\n",
    "\n",
    "\n",
    "In a high dimensional dataset, there remain some entirely irrelevant, insignificant and unimportant features. It has been seen that the contribution of these types of features is often less towards predictive modeling as compared to the critical features. They may have zero contribution as well. These features cause a number of problems which in turn prevents the process of efficient predictive modeling\n",
    "\n",
    "- Unnecessary resource allocation for these features.\n",
    "- These features act as a noise for which the machine learning model can perform terribly poorly.\n",
    "- The machine model takes more time to get trained.\n",
    "\n",
    "The most economical solution is **Feature Selection**. Feature Selection is the process of selecting out the most significant features from a given dataset. In many of the cases, Feature Selection can enhance the performance of a machine learning model as well. \n",
    "\n",
    "\"The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data.\"\n",
    "\n",
    "the importance of feature selection: \n",
    " - It enables the machine learning algorithm to train faster.\n",
    " - It reduces the complexity of a model and makes it easier to interpret.\n",
    " - It improves the accuracy of a model if the right subset is chosen.\n",
    " - It reduces Overfitting.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  **1. difference between dimensionality reduction and feature selection. **\n",
    "  a dimensionality reduction method does so by creating new combinations of attributes (sometimes known as feature transformation), whereas feature selection methods include and exclude attributes present in the data without changing them.\n",
    "     - dimensionality reduction methods are Principal Component Analysis, Singular Value Decomposition, Linear Discriminant Analysis, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **2. different types of general feature selection methods** - Filter methods, Wrapper methods, and Embedded methods.\n",
    " - 1) fitler methods:\n",
    "  Filter methods are generally used as a data preprocessing step. The selection of features is independent of any machine learning algorithm. Features give rank on the basis of statistical scores which tend to determine the features' correlation with the outcome variable.\n",
    "  Statistical tests can be used to select those features that have the strongest relationships with the output variable.\n",
    "\n",
    "\n",
    "     - Some examples of some filter methods include the Chi-squared test, information gain, and correlation    coefficient scores.\n",
    "     \n",
    "  - 2) wrapper method: a wrapper method needs one machine learning algorithm and uses its performance as evaluation criteria. Some typical examples of wrapper methods are forward feature selection, backward feature elimination, recursive feature elimination, etc. \n",
    "     - Recursive Feature elimination: Recursive feature elimination performs a greedy search to find the best performing feature subset.\n",
    "   \n",
    "   \n",
    "   - 3) Embedded methods: are iterative in a sense that takes care of each iteration of the model training process and carefully extract those features which contribute the most to the training for a particular iteration. Regularization methods are the most commonly used embedded methods which penalize a feature given a coefficient threshold. Examples of regularization algorithms are the LASSO, Elastic Net, Ridge Regression, etc.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "%%html\n",
    "### fitler methods\n",
    "<img src = 'filter.png', width = 300, height = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "https://www.datacamp.com/community/tutorials/feature-selection-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ** Difference between filter and wrapper methods **\n",
    "\n",
    " - Filter methods do not incorporate a machine learning model in order to determine if a feature is good or bad whereas wrapper methods use a machine learning model and train it the feature to decide if it is essential or not.\n",
    " - Filter methods are much faster compared to wrapper methods as they do not involve training the models. On the other hand, wrapper methods are computationally costly, and in the case of massive datasets, wrapper methods are not the most effective feature selection method to consider.\n",
    " - Filter methods may fail to find the best subset of features in situations when there is not enough data to model the statistical correlation of the features, but wrapper methods can always provide the best subset of features because of their exhaustive nature.\n",
    " - Using features from wrapper methods in your final machine learning model can lead to overfitting as wrapper methods already train machine learning models with the features and it affects the true power of learning. But the features from filter methods will not lead to overfitting in most of the cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example 1: use random forest and sklearn.feature_selection.SelectFromModel to do feature selection\n",
    "\n",
    "https://hub.packtpub.com/4-ways-implement-feature-selection-python-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "\n",
    "\n",
    "# Import the necessary modules\n",
    "from time import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "\n",
    "start = time()\n",
    "\n",
    "\n",
    "# feature selection using lassoCV\n",
    "clf = LassoCV(cv=5)\n",
    "sfm = SelectFromModel(clf)\n",
    "\n",
    "# using anova for feature selection\n",
    "anova_filter = SelectKBest(f_regression)\n",
    "mutual = SelectKBest(mutual_info_regression)\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [('scaler', StandardScaler()),\n",
    "         ('filter', sfm),\n",
    "#          ('ridge', Ridge())\n",
    "        ('rf', RandomForestRegressor())\n",
    "        ]\n",
    "\n",
    "# Create the pipeline: pipeline \n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# # Specify the hyperparameter space\n",
    "# parameters = {\n",
    "#              'ridge__alpha':[1e-2, 1, 5, 10, 20, 30, 40]\n",
    "# }\n",
    "\n",
    "# Specify the hyperparameter space\n",
    "parameters = {\n",
    "#                 'filter__k': [10, 20 ,30],\n",
    "              'rf__n_estimators':[200, 500, 1000]\n",
    "#               ,'rf__max_features':['auto','sqrt']\n",
    "             }\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_refund, test_size = 0.4, random_state = 42)\n",
    "\n",
    "# Create the GridSearchCV object: gm_cv\n",
    "gm_cv = GridSearchCV(pipeline, param_grid= parameters, cv = 5)\n",
    "\n",
    "# Fit to the training set\n",
    "gm_cv.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print the metrics\n",
    "r2 = gm_cv.score(X_test, y_test)\n",
    "y_pred = gm_cv.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Tuned Alpha: {}\".format(gm_cv.best_params_))\n",
    "print(\"Tuned R squared: {}\".format(r2))\n",
    "print(\"Tuned MSE: {}\".format(mse))\n",
    "\n",
    "print(\"this takes %.2f seconds\" %(time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regressor = gm_cv.best_estimator_.named_steps['rf']\n",
    "selector = gm_cv.best_estimator_.named_steps['filter']\n",
    "print ('after selection, num of feature %d' %selector.get_support().sum())\n",
    "print ('original feature: %d' %X_final.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to get what features are selected or removed\n",
    "pd.DataFrame(sorted(zip(selector.get_support(), X_train.columns)\n",
    "                          , key=lambda x: x[0], reverse = True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to plot feature_importance.\n",
    "def plot_feature_importance(feature_importances,feature_names, title):\n",
    "    ftr_imp_df = pd.DataFrame(sorted(zip(feature_names,feature_importances)\n",
    "                          , key=lambda x: x[1], reverse = False)\n",
    "                   )\n",
    "    y_pos = np.arange(ftr_imp_df.shape[0])\n",
    "\n",
    "    plt.barh(y_pos, ftr_imp_df[1], align='center', alpha=0.4)\n",
    "    plt.yticks(y_pos, ftr_imp_df[0])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(title)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "plt.subplots(figsize=(15,10))    \n",
    "rf = gm_cv.best_estimator_.named_steps['rf']\n",
    "selector = gm_cv.best_estimator_.named_steps['filter']\n",
    "feature_importances = rf.feature_importances_\n",
    "feature_names = X_train.columns[selector.get_support() == True]\n",
    "\n",
    "plot_feature_importance(feature_importances,feature_names, title = 'random forest ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's checked these selected features' pvalue with response\n",
    "\n",
    "feature_names = X_train.columns[selector.get_support() == True]\n",
    "\n",
    "f_test, p_val = f_regression(X_final[feature_names], y_refund)\n",
    "print ('p-value after Anova with response')\n",
    "df = pd.DataFrame(sorted(zip(p_val,feature_names)\n",
    "                          , key=lambda x: x[0], reverse = False))\n",
    "df['significant'] = np.where(df[0] <0.05, 'sig', 'not sig') \n",
    "df"
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
