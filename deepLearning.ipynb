{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** intro to deep learning using keras **\n",
    "https://campus.datacamp.com/courses/deep-learning-in-python/fine-tuning-keras-models?ex=13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep Learning With Jupyter Notebooks In The Cloud\n",
    "   - This step-by-step tutorial will show you how to set up and use Jupyter Notebook on Amazon Web Services (AWS) EC2 GPU for deep learning.\n",
    "https://www.datacamp.com/community/tutorials/deep-learning-jupyter-aws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Deep learning models capture interactions\n",
    "\n",
    "    - linear regression doesn't consider interactions between features\n",
    "    - Neural networks account for interactions really well\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "### deep learning interaction\n",
       "<img src = 'interaction.png', width = 300, height =200>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "### deep learning interaction\n",
    "<img src = 'interaction.png', width = 300, height =200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> part 1. forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "### forward propagation\n",
       "<img src = 'forward.png', width = 300, height =200>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "### forward propagation\n",
    "<img src = 'forward.png', width = 300, height =200>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "### multiply layers\n",
       "<img src = 'multiple.png', width = 300, height =200>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "### multiply layers\n",
    "<img src = 'multiple.png', width = 300, height =200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "- do multiply addition in hidden layers\n",
    "- activiation function on hidden layers \n",
    "  - activation function allows model to capture non-linearility\n",
    "  - apply to node inputs to produce node outputs\n",
    "  - ** ReLU **(Rectified Linear Activation)-- standard industry/ tanh()\n",
    "      - if x <0, ReLU(x) = 0\n",
    "      - if x >0, ReLU(x) = x\n",
    "  \n",
    "\n",
    "\n",
    "### deep learning(also called representation learning): - multiply hidden layers\n",
    "    - deep networks internally build representations of pattern in the data\n",
    "    - partially replace the need for feature engineering\n",
    "    - subsequent layers build increasingly sophisticated representations of raw data\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- example 1: dot product + ReLU for one row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(input):\n",
    "    '''Define your relu activation function here'''\n",
    "    # Calculate the value for the output of the relu function: output\n",
    "    output = max(0, input)\n",
    "    \n",
    "    # Return the value just calculated\n",
    "    return(output)\n",
    "\n",
    "# Calculate node 0 value: node_0_output\n",
    "node_0_input = (input_data * weights['node_0']).sum()\n",
    "node_0_output = relu(node_0_input)\n",
    "\n",
    "# Calculate node 1 value: node_1_output\n",
    "node_1_input = (input_data * weights['node_1']).sum()\n",
    "node_1_output = relu(node_1_input)\n",
    "\n",
    "# Put node values into array: hidden_layer_outputs\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "\n",
    "# Calculate model output (do not apply relu)\n",
    "model_output = (hidden_layer_outputs * weights['output']).sum()\n",
    "\n",
    "# Print model output\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- example 2: make a function and predict multiple rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define predict_with_network()\n",
    "def predict_with_network(input_data_row, weights):\n",
    "\n",
    "    # Calculate node 0 value\n",
    "    node_0_input = (input_data_row * weights['node_0']).sum()\n",
    "    node_0_output = relu(node_0_input)\n",
    "\n",
    "    # Calculate node 1 value\n",
    "    node_1_input = (input_data_row * weights['node_1']).sum()\n",
    "    node_1_output = relu(node_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_layer_outputs\n",
    "    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "    \n",
    "    # Calculate model output\n",
    "    input_to_final_layer = (hidden_layer_outputs * weights['output']).sum()\n",
    "    model_output = relu(input_to_final_layer)\n",
    "    \n",
    "    # Return model output\n",
    "    return(model_output)\n",
    "\n",
    "\n",
    "# Create empty list to store prediction results\n",
    "results = []\n",
    "for input_data_row in input_data:\n",
    "    # Append prediction to results\n",
    "    results.append(predict_with_network(input_data_row, weights))\n",
    "\n",
    "# Print results\n",
    "print(results)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- example with 2 hidden layers\n",
    "%%html\n",
    "### two layers\n",
    "<img src = 'two_layer.png', width = 300, height =200>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_with_network(input_data):\n",
    "    # Calculate node 0 in the first hidden layer\n",
    "    node_0_0_input = (input_data * weights['node_0_0']).sum()\n",
    "    node_0_0_output = relu(node_0_0_input )\n",
    "\n",
    "    # Calculate node 1 in the first hidden layer\n",
    "    node_0_1_input = (input_data * weights['node_0_1']).sum()\n",
    "    node_0_1_output = relu(node_0_1_input )\n",
    "\n",
    "    # Put node values into array: hidden_0_outputs\n",
    "    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])\n",
    "    \n",
    "    # Calculate node 0 in the second hidden layer\n",
    "    node_1_0_input = (hidden_0_outputs * weights['node_1_0']).sum()\n",
    "    node_1_0_output = relu(node_1_0_input)\n",
    "\n",
    "    # Calculate node 1 in the second hidden layer\n",
    "    node_1_1_input = (hidden_0_outputs * weights['node_1_1']).sum()\n",
    "    node_1_1_output = relu(node_1_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_1_outputs\n",
    "    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])\n",
    "\n",
    "    # Calculate model output: model_output\n",
    "    model_output = (hidden_1_outputs * weights['output']).sum()\n",
    "    \n",
    "    # Return model_output\n",
    "    return(model_output)\n",
    "\n",
    "output = predict_with_network(input_data)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> part 2. Optimization\n",
    "   - Loss function: Aggregates errors in prediction from many data points to a single number\n",
    "        - for example, one common loss function for regression is MSE(mean square error)\n",
    "   - find weights that give the lowest value for loss function: gradient descent\n",
    "        - solution: learning rate-- update each weight by substracting learning_rate * slope\n",
    "   - backpropogation:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "### backpropagation\n",
       "<img src = 'back.png', width = 500, height =400>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%%html\n",
    "### backpropagation\n",
    "<img src = 'back.png', width = 500, height =400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "### stok\n",
       "<img src = 'stok.png', width = 500, height =400>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "### stochastic gradient descent\n",
    "<img src = 'stok.png', width = 500, height =400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** gradient descent **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "### slope of weights\n",
       "<img src = 'slope1.png', width = 500, height =400>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "### slope of weights\n",
    "<img src = 'slope1.png', width = 500, height =400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "### slope of weights\n",
       "<img src = 'back_prop.png', width = 800, height =600>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "### slope of weights\n",
    "<img src = 'back_prop.png', width = 800, height =600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate slope: gradient = 2 * input_data * error\n",
    "\n",
    "# Calculate the predictions: preds\n",
    "preds = (input_data * weights).sum()\n",
    "\n",
    "# Calculate the error: error\n",
    "error = target - preds \n",
    "\n",
    "# Calculate the slope: slope\n",
    "slope = 2 * input_data * error\n",
    "\n",
    "# Print the slope\n",
    "print(slope)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "- example 1: improve model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the learning rate: learning_rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Calculate the predictions: preds\n",
    "preds = (weights * input_data).sum()\n",
    "\n",
    "# Calculate the error: error\n",
    "error = preds - target\n",
    "\n",
    "# Calculate the slope: slope\n",
    "slope = 2 * input_data * error\n",
    "\n",
    "# Update the weights: weights_updated\n",
    "weights_updated = weights - slope * learning_rate\n",
    "\n",
    "# Get updated predictions: preds_updated\n",
    "preds_updated = (weights_updated * input_data).sum()\n",
    "\n",
    "# Calculate updated error: error_updated\n",
    "error_updated = preds_updated- target\n",
    "\n",
    "# Print the original error\n",
    "print(error)\n",
    "\n",
    "# Print the updated error\n",
    "print(error_updated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Making multiple updates to weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_updates = 20\n",
    "mse_hist = []\n",
    "\n",
    "# Iterate over the number of updates\n",
    "for i in range(n_updates):\n",
    "    # Calculate the slope: slope\n",
    "    slope = get_slope(input_data, target, weights)\n",
    "    \n",
    "    # Update the weights: weights\n",
    "    weights = weights - slope * 0.01\n",
    "    \n",
    "    # Calculate mse with new weights: mse\n",
    "    mse = get_mse(input_data, target, weights)\n",
    "    \n",
    "    # Append the mse to mse_hist\n",
    "    mse_hist.append(mse)\n",
    "\n",
    "# Plot the mse history\n",
    "plt.plot(mse_hist)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ** part 3. Keras **\n",
    "\n",
    "  - create a model\n",
    "  - compile and fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "print (\"TensorFlow version: \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "print (n_cols)\n",
    "# Set up the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n",
    "\n",
    "# Add the second layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer = 'adam', loss = 'mean_sqaure_error')\n",
    "\n",
    "# fit\n",
    "model.fit(predictors, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2) compile the model\n",
    "    - specify the optimizer\n",
    "       - control learning rate\n",
    "       - many options and mathematically complex\n",
    "       - 'adam' is a good one to use\n",
    "    - loss function\n",
    "       - 'mean square error' is a common one for regression\n",
    "       - 'categorical_crossentropy' for classification (similar to log-loss): lower is better\n",
    "        \n",
    "- 3) fit the model\n",
    "    - apply backpropagation and gradient descent with your data to update the weights\n",
    "    - scaling data(standard scaler) before fitting can ease optimization\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### classification\n",
    "   - 'categorical_crossentropy' for classification (similar to log-loss): lower is better\n",
    "   - add metrics = 'accuracy' to compile step for easy-to-understand diagonitics\n",
    "   - Output layer has separate node for each possible outcome and uses 'softmax' activation\n",
    "      - softmax activation ensures all prediction outcomes will sum to 1, so we can use prediction as propabilities.\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert the target to categorical: target\n",
    "target = to_categorical(df.survived)\n",
    "\n",
    "# Set up the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(32, activation = 'relu', input_shape = (n_cols,)))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(2,activation = 'softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = 'sgd'\n",
    "              , loss = 'categorical_crossentropy'\n",
    "              , metrics = ['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(predictors, target)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example 2: optimizer the compiler by different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the SGD optimizer\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Create list of learning rates: lr_to_test\n",
    "lr_to_test = [0.000001, 0.01, 1]\n",
    "\n",
    "# Loop over learning rates\n",
    "for lr in lr_to_test:\n",
    "    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n",
    "    \n",
    "    # Build new model to test, unaffected by previous models\n",
    "    model = get_new_model()\n",
    "    \n",
    "    # Create SGD optimizer with specified learning rate: my_optimizer\n",
    "    my_optimizer = SGD(lr = lr)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer = my_optimizer\n",
    "                  , loss = 'categorical_crossentropy')\n",
    "    \n",
    "    # test and train split and Fit the model \n",
    "    model.fit(predictors, target, validation_split = 0.3)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ###part 4. tuning the model\n",
    "   - 1)dying neuron problem\n",
    "      - a node starts to get negative inputs, it may continue only getting negative inputs\n",
    "      - slope of weights, the gradient, is always 0. (due to slope of activation = 0)\n",
    "      - if this happens, change activation function as a solution.\n",
    "   - 2) model validation in deep learning\n",
    "      - commonly use model validation split(train_test_split) rather than cross-validation.\n",
    "      - deep learning widely used in large datasets\n",
    "      - single validation score is based on large dataset and it is reliable.\n",
    "      - repeated training from cross-validation is time-consuming.\n",
    "   - 3) early stopping\n",
    "      - early stopping if there is no improvement of metrics\n",
    "         - use early stopping to stop optimization when it isn't helping any more. Since the optimization stops automatically when it isn't helping, you can also set a high value for epochs in your call to .fit()\n",
    "      - based on this, we can experiment with different architectures: more/fewer layers, more/fewer nodes, etc.\n",
    "      - create a great model needs experimentation\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import EarlyStopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "# Specify the model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape = input_shape))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience = 2)\n",
    "\n",
    "# Fit the model with early stopping and higher epochs threshold\n",
    "model.fit(predictors, target, validation_split= 0.3, epochs = 30, callbacks = [early_stopping_monitor])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- example 3: experiemnt two models and compare scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "# Create the new model: model_2\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Add the first and second layers \n",
    "# model_1 only two layers with 10 units\n",
    "model_2.add(Dense(100, activation='relu', input_shape = input_shape))\n",
    "model_2.add(Dense(100, activation='relu'))\n",
    "\n",
    "\n",
    "# # OR Add the first and second layers\n",
    "# model_2.add(Dense(50, activation='relu', input_shape = input_shape))\n",
    "# model_2.add(Dense(50, activation='relu'))\n",
    "# model_2.add(Dense(50, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model_2.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_2.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fit model_1\n",
    "model_1_training = model_1.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Fit model_2\n",
    "model_2_training = model_2.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Model capacity\n",
    "      - validation score(test data score) is the ultimate measure of a model performance.\n",
    "      - model capacity --> model complexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "### slope of weights\n",
       "<img src = 'capacity.png', width = 400, height =200>\n",
       "### deeplearning capcity experimentation example\n",
       "<img src = 'cap_example.png', width = 400, height =200>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "### slope of weights\n",
    "<img src = 'capacity.png', width = 400, height =200>\n",
    "### deeplearning capcity experimentation example\n",
    "<img src = 'cap_example.png', width = 400, height =200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- example 3. Building your own digit recognition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(50, activation = 'relu', input_shape = (784,)))\n",
    "\n",
    "# Add the second hidden layer\n",
    "model.add(Dense(50, activation = 'relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X,y,validation_split = 0.3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
